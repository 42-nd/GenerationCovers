{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y6kuhcI0LIjn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import log2\n",
        "\n",
        "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G0X9J-AYU6w",
        "outputId": "261e5bee-71c4-4d8d-e03b-a4d5f03671b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pgZaJ-kqLOW2"
      },
      "outputs": [],
      "source": [
        "class WSConv2d(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2\n",
        "    ):\n",
        "        super(WSConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.scale = (gain / (in_channels * (kernel_size**2))) ** 0.5\n",
        "        self.bias = self.conv.bias\n",
        "        self.conv.bias = None\n",
        "\n",
        "        nn.init.normal_(self.conv.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XXXkQnGbLR3Z"
      },
      "outputs": [],
      "source": [
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PixelNorm, self).__init__()\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zy91tVmoLTCu"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.use_pn = use_pixelnorm\n",
        "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
        "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "        self.pn = PixelNorm()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky(self.conv1(x))\n",
        "        x = self.pn(x) if self.use_pn else x\n",
        "        x = self.leaky(self.conv2(x))\n",
        "        x = self.pn(x) if self.use_pn else x\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XQ28RuY2LVci"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self, z_dim, in_channels, num_classes, img_size, embed_size, img_channels=3\n",
        "    ):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.initial = nn.Sequential(\n",
        "            PixelNorm(),\n",
        "            nn.ConvTranspose2d(z_dim + embed_size, in_channels, 4, 1, 0),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            PixelNorm(),\n",
        "        )\n",
        "\n",
        "        self.initial_rgb = WSConv2d(\n",
        "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
        "        )\n",
        "        self.prog_blocks, self.rgb_layers = (\n",
        "            nn.ModuleList([]),\n",
        "            nn.ModuleList([self.initial_rgb]),\n",
        "        )\n",
        "\n",
        "        for i in range(len(factors) - 1):\n",
        "            conv_in_c = int(in_channels * factors[i])\n",
        "            conv_out_c = int(in_channels * factors[i + 1])\n",
        "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
        "            self.rgb_layers.append(\n",
        "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
        "            )\n",
        "        self.embed = nn.Embedding(num_classes, embed_size)\n",
        "\n",
        "    def fade_in(self, alpha, upscaled, generated):\n",
        "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
        "\n",
        "    def forward(self, x, alpha, steps, labels):\n",
        "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
        "        x = torch.cat([x, embedding], dim=1)\n",
        "        out = self.initial(x)\n",
        "\n",
        "        if steps == 0:\n",
        "            return self.initial_rgb(out)\n",
        "\n",
        "        for step in range(steps):\n",
        "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
        "            out = self.prog_blocks[step](upscaled)\n",
        "\n",
        "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
        "        final_out = self.rgb_layers[steps](out)\n",
        "        return self.fade_in(alpha, final_upscaled, final_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U5Dx8Ve5LYL6"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, z_dim, in_channels, img_size, num_classes, img_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "\n",
        "        for i in range(len(factors) - 1, 0, -1):\n",
        "            conv_in = int(in_channels * factors[i])\n",
        "            conv_out = int(in_channels * factors[i - 1])\n",
        "            self.prog_blocks.append(ConvBlock(conv_in, conv_out, use_pixelnorm=False))\n",
        "            self.rgb_layers.append(\n",
        "                WSConv2d(img_channels + 1, conv_in, kernel_size=1, stride=1, padding=0)\n",
        "            )\n",
        "\n",
        "        self.initial_rgb = WSConv2d(\n",
        "            img_channels + 1, in_channels, kernel_size=1, stride=1, padding=0\n",
        "        )\n",
        "        self.rgb_layers.append(self.initial_rgb)\n",
        "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.final_block = nn.Sequential(\n",
        "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WSConv2d(in_channels, 1, kernel_size=1, padding=0, stride=1),\n",
        "        )\n",
        "        self.embed = nn.Embedding(num_classes, img_size * img_size)\n",
        "\n",
        "    def fade_in(self, alpha, downscaled, out):\n",
        "        return alpha * out + (1 - alpha) * downscaled\n",
        "\n",
        "    def minibatch_std(self, x):\n",
        "        batch_statistics = (\n",
        "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
        "        )\n",
        "\n",
        "        return torch.cat([x, batch_statistics], dim=1)\n",
        "\n",
        "    def forward(self, x, alpha, steps, labels):\n",
        "        embedding = self.embed(labels).view(\n",
        "            labels.shape[0], 1, self.img_size, self.img_size\n",
        "        )\n",
        "        x = torch.cat([x, embedding], dim=1)\n",
        "        cur_step = len(self.prog_blocks) - steps\n",
        "\n",
        "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
        "\n",
        "        if steps == 0:\n",
        "            out = self.minibatch_std(out)\n",
        "            return self.final_block(out).view(out.shape[0], -1)\n",
        "\n",
        "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
        "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
        "\n",
        "        out = self.fade_in(alpha, downscaled, out)\n",
        "\n",
        "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
        "            out = self.prog_blocks[step](out)\n",
        "            out = self.avg_pool(out)\n",
        "\n",
        "        out = self.minibatch_std(out)\n",
        "        return self.final_block(out).view(out.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MZLoBR5YLa8-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "\n",
        "START_TRAIN_AT_IMG_SIZE = 4\n",
        "DATASET = \"\"\n",
        "SL_PATH = \"\"\n",
        "CHECKPOINT_GEN = \"generator.pth\"\n",
        "CHECKPOINT_CRITIC = \"critic.pth\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SAVE_MODEL = True\n",
        "LOAD_MODEL = False\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZES = [64, 64, 32, 32, 16, 8, 4]  # MOST OPTIMAL\n",
        "# BATCH_SIZES = [256, 256, 256, 128, 128, 64, 32]\n",
        "# BATCH_SIZES = [128, 128, 128, 64, 64, 32, 16]\n",
        "SIZES = [4, 8, 16, 32, 64, 128, 256]\n",
        "CHANNELS_IMG = 3\n",
        "GEN_EMBEDDING = 128\n",
        "NUM_CLASSES = 19\n",
        "Z_DIM = 256\n",
        "IN_CHANNELS = 256\n",
        "CRITIC_ITERATIONS = 1\n",
        "LAMBDA_GP = 10\n",
        "PROGRESSIVE_EPOCHS = [30] * len(BATCH_SIZES)\n",
        "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n",
        "NUM_WORKERS = 2\n",
        "genre_mapping = {  # just for this current dataset\n",
        "    \"rap\": 0,\n",
        "    \"pop\": 1,\n",
        "    \"folk\": 2,\n",
        "    \"electronics\": 3,\n",
        "    \"rock\": 4,\n",
        "    \"house\": 5,\n",
        "    \"dance\": 6,\n",
        "    \"jazz\": 7,\n",
        "    \"alternative\": 8,\n",
        "    \"newage\": 9,\n",
        "    \"relax\": 10,\n",
        "    \"classical\": 11,\n",
        "    \"soul\": 12,\n",
        "    \"reggae\": 13,\n",
        "    \"indie\": 14,\n",
        "    \"techno\": 15,\n",
        "    \"country\": 16,\n",
        "    \"bard\": 17,\n",
        "    \"rnb\": 18,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tUswBF7yLmIU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "\n",
        "def plot_to_tensorboard(writer, loss_critic, loss_gen, real, fake, tensorboard_step):\n",
        "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
        "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
        "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
        "\n",
        "\n",
        "def gradient_penalty(critic, labels, real, fake, alpha, train_step, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    mixed_scores = critic(interpolated_images, alpha, train_step, labels=labels)\n",
        "\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    checkpoint_path = os.path.join(SL_PATH, filename)\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint_path = os.path.join(SL_PATH, checkpoint_file)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=\"cuda\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def generate_examples(gen, steps, size, message, truncation=0.7, n=100):\n",
        "    gen.eval()\n",
        "    alpha = 1.0\n",
        "    for i in range(n):\n",
        "        with torch.no_grad():\n",
        "            noise = torch.tensor(\n",
        "                truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)),\n",
        "                device=DEVICE,\n",
        "                dtype=torch.float32,\n",
        "            )\n",
        "            img = gen(noise, alpha, steps)\n",
        "            save_image(img * 0.5 + 0.5, f\"saved_examples/img_{size}_{i}_{message}.png\")\n",
        "    gen.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kk4BUHS6Lz4K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from math import log2\n",
        "from PIL import Image\n",
        "\n",
        "torch.backends.cudnn.benchmarks = True\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_folder, image_size, transform=None):\n",
        "        self.root_folder = root_folder\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((image_size, image_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.Normalize(\n",
        "                    [0.5 for _ in range(CHANNELS_IMG)],\n",
        "                    [0.5 for _ in range(CHANNELS_IMG)],\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        self.images = self._get_images()\n",
        "\n",
        "    def _get_images(self):\n",
        "        images = []\n",
        "        try:\n",
        "            images.extend(\n",
        "                [\n",
        "                    filename\n",
        "                    for filename in os.listdir(self.root_folder)\n",
        "                    if filename.endswith(\".png\") or filename.endswith(\".jpg\")\n",
        "                ]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error while listing files in {self.root_folder}: {e}\")\n",
        "        return images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_folder, self.images[idx])\n",
        "        try:\n",
        "            img = Image.open(img_path)\n",
        "            label_str = os.path.basename(img_path).split(\"_\")[-1].split(\".\")[0]\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            return img, label_str\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "\n",
        "def get_loader(image_size):\n",
        "    batch_size = BATCH_SIZES[SIZES.index(START_TRAIN_AT_IMG_SIZE)]\n",
        "    dataset = CustomDataset(\n",
        "        DATASET, image_size=image_size, transform=transforms.ToTensor()\n",
        "    )\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    return loader, dataset\n",
        "\n",
        "\n",
        "def train_fn(\n",
        "    critic,\n",
        "    gen,\n",
        "    loader,\n",
        "    dataset,\n",
        "    step,\n",
        "    alpha,\n",
        "    opt_critic,\n",
        "    opt_gen,\n",
        "    tensorboard_step,\n",
        "    writer,\n",
        "    scaler_gen,\n",
        "    scaler_critic,\n",
        "):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    for batch_idx, (real, labels) in enumerate(loop):\n",
        "        real = real.to(DEVICE)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        labels = [genre_mapping[genre] for genre in labels]\n",
        "        labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            fake = gen(noise, alpha, step, labels)\n",
        "            critic_real = critic(real, alpha, step, labels)\n",
        "            critic_fake = critic(fake.detach(), alpha, step, labels)\n",
        "            gp = gradient_penalty(\n",
        "                critic, labels, real, fake, alpha, step, device=DEVICE\n",
        "            )\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "                + LAMBDA_GP * gp\n",
        "                + (0.001 * torch.mean(critic_real**2))\n",
        "            )\n",
        "\n",
        "        opt_critic.zero_grad()\n",
        "        scaler_critic.scale(loss_critic).backward()\n",
        "        scaler_critic.step(opt_critic)\n",
        "        scaler_critic.update()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            gen_fake = critic(fake, alpha, step, labels)\n",
        "            loss_gen = -torch.mean(gen_fake)\n",
        "\n",
        "        opt_gen.zero_grad()\n",
        "        scaler_gen.scale(loss_gen).backward()\n",
        "        scaler_gen.step(opt_gen)\n",
        "        scaler_gen.update()\n",
        "\n",
        "        alpha += cur_batch_size / ((PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset))\n",
        "        alpha = min(alpha, 1)\n",
        "\n",
        "        if batch_idx % 500 == 0:\n",
        "            with torch.no_grad():\n",
        "                fixed_fakes = gen(noise, alpha, step, labels) * 0.5 + 0.5\n",
        "            plot_to_tensorboard(\n",
        "                writer,\n",
        "                loss_critic.item(),\n",
        "                loss_gen.item(),\n",
        "                real.detach(),\n",
        "                fixed_fakes.detach(),\n",
        "                tensorboard_step,\n",
        "            )\n",
        "            tensorboard_step += 1\n",
        "\n",
        "        loop.set_postfix(\n",
        "            gp=gp.item(),\n",
        "            loss_critic=loss_critic.item(),\n",
        "        )\n",
        "\n",
        "    return tensorboard_step, alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V5UiScFvMHAM"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    gen = Generator(\n",
        "        Z_DIM,\n",
        "        IN_CHANNELS,\n",
        "        img_channels=CHANNELS_IMG,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        img_size=START_TRAIN_AT_IMG_SIZE,\n",
        "        embed_size=GEN_EMBEDDING,\n",
        "    ).to(DEVICE)\n",
        "    critic = Discriminator(\n",
        "        Z_DIM,\n",
        "        IN_CHANNELS,\n",
        "        img_channels=CHANNELS_IMG,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        img_size=START_TRAIN_AT_IMG_SIZE,\n",
        "    ).to(DEVICE)\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
        "    opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n",
        "    scaler_critic = torch.cuda.amp.GradScaler()\n",
        "    scaler_gen = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # for tensorboard plotting\n",
        "    writer = SummaryWriter(f\"logs/gan1\")\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_GEN,\n",
        "            gen,\n",
        "            opt_gen,\n",
        "            LEARNING_RATE,\n",
        "        )\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_CRITIC,\n",
        "            critic,\n",
        "            opt_critic,\n",
        "            LEARNING_RATE,\n",
        "        )\n",
        "    gen.train()\n",
        "    critic.train()\n",
        "\n",
        "    tensorboard_step = 0\n",
        "    step = SIZES.index(START_TRAIN_AT_IMG_SIZE)\n",
        "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
        "        alpha = 1e-5\n",
        "        loader, dataset = get_loader(SIZES[step])\n",
        "        print(f\"Current image size: {SIZES[step]}\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            tensorboard_step, alpha = train_fn(\n",
        "                critic,\n",
        "                gen,\n",
        "                loader,\n",
        "                dataset,\n",
        "                step,\n",
        "                alpha,\n",
        "                opt_critic,\n",
        "                opt_gen,\n",
        "                tensorboard_step,\n",
        "                writer,\n",
        "                scaler_gen,\n",
        "                scaler_critic,\n",
        "            )\n",
        "\n",
        "            if SAVE_MODEL:\n",
        "                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
        "                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
        "            # if epoch % 1 == 0:\n",
        "            #   generate_examples(gen,size = (SIZES[step]),steps=step,n=3, message = epoch)\n",
        "\n",
        "        # generate_examples(gen,size = (SIZES[step]),steps=step,n=3,message = \"epoch ended\")\n",
        "        step += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xt8vbD3MI_K",
        "outputId": "575c3124-cac0-4614-d35d-3761f897077c"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSTO7nY3Z141"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/ --port=6010"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
